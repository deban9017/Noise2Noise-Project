{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":11358021,"sourceType":"datasetVersion","datasetId":7107972}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"\n# Supervised Image Denoising CNN\n\nThis notebook trains a simple CNN model to denoise images using a supervised approach.\n1. Load and preprocess clean training images, adding synthetic noise.\n2. Train the CNN on the noisy/clean image pairs and save the trained model.\n3. Load the saved model and test it on a new image (clean or noisy).","metadata":{}},{"cell_type":"markdown","source":"## Section 1: Data Loading and Preprocessing","metadata":{}},{"cell_type":"code","source":"import os\nimport glob\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm # Use standard tqdm if not in notebook\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nimport torchvision.transforms.functional as TF","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:10:34.609586Z","iopub.execute_input":"2025-04-13T05:10:34.609754Z","iopub.status.idle":"2025-04-13T05:10:42.334086Z","shell.execute_reply.started":"2025-04-13T05:10:34.609738Z","shell.execute_reply":"2025-04-13T05:10:42.333414Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"# --- Parameters ---\nTRAIN_IMG_DIR = '/kaggle/input/aiml-project/train_images_500' # Directory containing clean training images\nTEST_IMG_DIR = '/kaggle/input/aiml-project/test_images_50'   # Directory containing test images (used in Section 3)\nMODEL_SAVE_DIR = '/kaggle/working/'             # Directory to save the trained model\n\nIMG_SIZE = 256          # Resize images to this size (height and width)\nBATCH_SIZE = 32         # Number of images per training batch\nLEARNING_RATE = 0.001\nEPOCHS = 50             # Number of training epochs (adjust as needed)\nNOISE_TYPE = 'poiss'    # Noise type for training data: 'gauss' or 'poiss'\nNOISE_LEVEL = 15        # Noise level for training (0-255 for Gauss, 0-1 scale for Poisson)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:10:42.334800Z","iopub.execute_input":"2025-04-13T05:10:42.335210Z","iopub.status.idle":"2025-04-13T05:10:42.339892Z","shell.execute_reply.started":"2025-04-13T05:10:42.335188Z","shell.execute_reply":"2025-04-13T05:10:42.339139Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"# Check for GPU\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"Using device: {device}\")\n\n# Create save directory if it doesn't exist\nos.makedirs(MODEL_SAVE_DIR, exist_ok=True)\nos.makedirs(TEST_IMG_DIR, exist_ok=True) # Also ensure test dir exists for later use","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:10:42.342062Z","iopub.execute_input":"2025-04-13T05:10:42.342355Z","iopub.status.idle":"2025-04-13T05:10:42.431487Z","shell.execute_reply.started":"2025-04-13T05:10:42.342335Z","shell.execute_reply":"2025-04-13T05:10:42.430680Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# --- Preprocessing and Noise Functions ---\n\ndef preprocess_image(image_path, img_size):\n    \"\"\"Loads, resizes, normalizes, and converts image to PyTorch tensor.\"\"\"\n    try:\n        img = Image.open(image_path).convert('RGB')\n        img = TF.resize(img, [img_size, img_size])\n        img_tensor = TF.to_tensor(img) # Converts to [C, H, W] and scales to [0.0, 1.0]\n        return img_tensor\n    except Exception as e:\n        print(f\"Error processing {image_path}: {e}\")\n        return None\n\ndef add_noise(x, noise_type='gauss', noise_level=25):\n    \"\"\"Adds Gaussian or Poisson noise to a clean image tensor (range [0, 1]).\"\"\"\n    if noise_type == 'gauss':\n        # Scale noise_level from 0-255 range to 0-1 range for std deviation\n        std_dev = noise_level / 255.0\n        noisy = x + torch.randn_like(x) * std_dev\n        noisy = torch.clamp(noisy, 0.0, 1.0)\n    elif noise_type == 'poiss':\n        # Poisson noise level often interpreted differently, assuming noise_level acts as a lambda multiplier\n        # Ensure input 'x' is scaled appropriately if needed, here assuming it's already 0-1\n        noisy = torch.poisson(noise_level * x) / noise_level\n        noisy = torch.clamp(noisy, 0.0, 1.0)\n    else:\n        print(f\"Warning: Unknown noise type '{noise_type}'. Returning original image.\")\n        noisy = x\n    return noisy\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:10:42.432216Z","iopub.execute_input":"2025-04-13T05:10:42.432431Z","iopub.status.idle":"2025-04-13T05:10:42.447557Z","shell.execute_reply.started":"2025-04-13T05:10:42.432413Z","shell.execute_reply":"2025-04-13T05:10:42.446729Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"# --- Custom Dataset ---\n\nclass DenoisingDataset(Dataset):\n    def __init__(self, clean_images_list, noise_type, noise_level):\n        self.clean_images = clean_images_list\n        self.noise_type = noise_type\n        self.noise_level = noise_level\n\n    def __len__(self):\n        return len(self.clean_images)\n\n    def __getitem__(self, idx):\n        clean_img = self.clean_images[idx]\n        noisy_img = add_noise(clean_img, self.noise_type, self.noise_level)\n        noisy_target = add_noise(clean_img, self.noise_type, self.noise_level)\n        return noisy_img, noisy_target","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:10:42.448483Z","iopub.execute_input":"2025-04-13T05:10:42.449213Z","iopub.status.idle":"2025-04-13T05:10:42.468690Z","shell.execute_reply.started":"2025-04-13T05:10:42.449190Z","shell.execute_reply":"2025-04-13T05:10:42.467764Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# --- Load Training Data ---\n\nprint(f\"Loading clean training images from: {TRAIN_IMG_DIR}\")\ntrain_image_paths = glob.glob(os.path.join(TRAIN_IMG_DIR, '*.[jJ][pP][gG]')) \\\n                   + glob.glob(os.path.join(TRAIN_IMG_DIR, '*.[pP][nN][gG]')) \\\n                   + glob.glob(os.path.join(TRAIN_IMG_DIR, '*.[tT][iI][fF][fF]')) # Add more extensions if needed\n\nif not train_image_paths:\n    print(f\"Error: No images found in {TRAIN_IMG_DIR}. Please check the path and image files.\")\n    # Stop execution or handle error appropriately in a real notebook\n    raise FileNotFoundError(f\"No training images found in {TRAIN_IMG_DIR}\")\n\nclean_train_tensors = []\nfor path in tqdm(train_image_paths, desc=\"Preprocessing Training Images\"):\n    tensor = preprocess_image(path, IMG_SIZE)\n    if tensor is not None:\n        clean_train_tensors.append(tensor)\n\nprint(f\"Successfully loaded and preprocessed {len(clean_train_tensors)} training images.\")\n\n# Create Dataset and DataLoader\ntrain_dataset = DenoisingDataset(clean_train_tensors, NOISE_TYPE, NOISE_LEVEL)\ntrain_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=0) # Adjust num_workers based on your system\n\n# Optional: Display a sample pair\nnoisy_sample, clean_sample = train_dataset[0]\nfig, ax = plt.subplots(1, 2)\nax[0].imshow(noisy_sample.permute(1, 2, 0))\nax[0].set_title('Noisy Sample')\nax[0].axis('off')\nax[1].imshow(clean_sample.permute(1, 2, 0))\nax[1].set_title('Clean Sample')\nax[1].axis('off')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:10:42.469633Z","iopub.execute_input":"2025-04-13T05:10:42.470358Z"}},"outputs":[{"name":"stdout","text":"Loading clean training images from: /kaggle/input/aiml-project/train_images_500\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Preprocessing Training Images:   0%|          | 0/500 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bc7496cc1084b4590ec5b7fd18109a7"}},"metadata":{}},{"name":"stdout","text":"Successfully loaded and preprocessed 500 training images.\n","output_type":"stream"}],"execution_count":null},{"cell_type":"markdown","source":"## Section 2: Model Training and Saving","metadata":{}},{"cell_type":"code","source":"# --- Define the CNN Model (same architecture as before) ---\nclass DenoisingCNN(nn.Module):\n    def __init__(self, n_chan=3, chan_embed=48):\n        super().__init__()\n        self.act = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n        self.conv1 = nn.Conv2d(n_chan, chan_embed, 3, padding=1)\n        self.conv2 = nn.Conv2d(chan_embed, chan_embed, 3, padding=1)\n        self.conv3 = nn.Conv2d(chan_embed, n_chan, 1) # Output has same channels as input\n\n    def forward(self, x):\n        # Input x: noisy image\n        x1 = self.act(self.conv1(x))\n        x2 = self.act(self.conv2(x1))\n        output = self.conv3(x2) # Predicts the clean image directly\n        # The original ZS-N2N predicted the residual (noise), here we predict the image\n        # Make sure the output range is sensible (e.g. clamping might be needed after)\n        return output\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Training Setup ---\nmodel = DenoisingCNN(n_chan=3).to(device) # Assuming 3 color channels\ncriterion = nn.MSELoss() # Mean Squared Error between predicted clean and actual clean\noptimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n# Optional: Learning rate scheduler\n# scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=EPOCHS // 3, gamma=0.5)\n\nprint(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad)}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# --- Training Loop ---\nprint(\"Starting Training...\")\nmodel.train() # Set model to training mode\nepoch_losses = []\n\nfor epoch in range(EPOCHS):\n    running_loss = 0.0\n    progress_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", leave=False)\n\n    for noisy_batch, clean_batch in progress_bar:\n        noisy_batch = noisy_batch.to(device)\n        clean_batch = clean_batch.to(device)\n\n        # Zero gradients\n        optimizer.zero_grad()\n\n        # Forward pass -> Predict clean image from noisy input\n        predicted_clean_batch = model(noisy_batch)\n\n        # Calculate loss\n        loss = criterion(predicted_clean_batch, clean_batch)\n\n        # Backward pass and optimize\n        loss.backward()\n        optimizer.step()\n\n        running_loss += loss.item() * noisy_batch.size(0) # loss.item() is avg loss per item in batch\n        progress_bar.set_postfix(loss=loss.item())\n\n    epoch_loss = running_loss / len(train_dataset)\n    epoch_losses.append(epoch_loss)\n    print(f\"Epoch {epoch+1}/{EPOCHS}, Average Loss: {epoch_loss:.6f}\")\n\n    # Step the scheduler (if using one)\n    # scheduler.step()\n\nprint(\"Training Finished.\")\n\n# Optional: Plot training loss\nplt.figure()\nplt.plot(range(1, EPOCHS + 1), epoch_losses)\nplt.xlabel('Epoch')\nplt.ylabel('Average MSE Loss')\nplt.title('Training Loss Curve')\nplt.grid(True)\nplt.show()\n","metadata":{"trusted":true,"execution":{"iopub.status.idle":"2025-04-13T05:28:36.931409Z","shell.execute_reply.started":"2025-04-13T05:10:47.427276Z","shell.execute_reply":"2025-04-13T05:28:36.930475Z"}},"outputs":[{"name":"stdout","text":"Epoch 1/50, Average Loss: 0.082640\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 2/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 2/50, Average Loss: 0.035183\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 3/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 3/50, Average Loss: 0.030882\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 4/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 4/50, Average Loss: 0.029293\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 5/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 5/50, Average Loss: 0.028587\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 6/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 6/50, Average Loss: 0.028166\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 7/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 7/50, Average Loss: 0.027916\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 8/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 8/50, Average Loss: 0.027755\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 9/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 9/50, Average Loss: 0.027637\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 10/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 10/50, Average Loss: 0.027534\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 11/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 11/50, Average Loss: 0.027460\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 12/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 12/50, Average Loss: 0.027405\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 13/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 13/50, Average Loss: 0.027346\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 14/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 14/50, Average Loss: 0.027299\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 15/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 15/50, Average Loss: 0.027266\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 16/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 16/50, Average Loss: 0.027226\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 17/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 17/50, Average Loss: 0.027185\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 18/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 18/50, Average Loss: 0.027149\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 19/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 19/50, Average Loss: 0.027114\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 20/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 20/50, Average Loss: 0.027087\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 21/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 21/50, Average Loss: 0.027066\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 22/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 22/50, Average Loss: 0.027033\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 23/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 23/50, Average Loss: 0.027004\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 24/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 24/50, Average Loss: 0.026986\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 25/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 25/50, Average Loss: 0.026960\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 26/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 26/50, Average Loss: 0.026937\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 27/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 27/50, Average Loss: 0.026925\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 28/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 28/50, Average Loss: 0.026900\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 29/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 29/50, Average Loss: 0.026878\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 30/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 30/50, Average Loss: 0.026848\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 31/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 31/50, Average Loss: 0.026835\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 32/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 32/50, Average Loss: 0.026822\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 33/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 33/50, Average Loss: 0.026791\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 34/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 34/50, Average Loss: 0.026771\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 35/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 35/50, Average Loss: 0.026753\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 36/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 36/50, Average Loss: 0.026731\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 37/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 37/50, Average Loss: 0.026718\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 38/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 38/50, Average Loss: 0.026690\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 39/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 39/50, Average Loss: 0.026677\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 40/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 40/50, Average Loss: 0.026650\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 41/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 41/50, Average Loss: 0.026647\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 42/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 42/50, Average Loss: 0.026629\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 43/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 43/50, Average Loss: 0.026612\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 44/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 44/50, Average Loss: 0.026597\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 45/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 45/50, Average Loss: 0.026568\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 46/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 46/50, Average Loss: 0.026543\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 47/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 47/50, Average Loss: 0.026520\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 48/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 48/50, Average Loss: 0.026508\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 49/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 49/50, Average Loss: 0.026489\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Epoch 50/50:   0%|          | 0/16 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Epoch 50/50, Average Loss: 0.026464\nTraining Finished.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"# --- Save the Trained Model ---\nmodel_save_filename = f\"{NOISE_TYPE}_cnn_n2n_nl{NOISE_LEVEL}.pth\"\nmodel_save_path = os.path.join(MODEL_SAVE_DIR, model_save_filename)\n\nprint(f\"Saving trained model to: {model_save_path}\")\ntorch.save(model.state_dict(), model_save_path)\nprint(\"Model saved successfully.\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:28:36.932222Z","iopub.execute_input":"2025-04-13T05:28:36.932488Z","iopub.status.idle":"2025-04-13T05:28:36.942898Z","shell.execute_reply.started":"2025-04-13T05:28:36.932469Z","shell.execute_reply":"2025-04-13T05:28:36.941917Z"}},"outputs":[{"name":"stdout","text":"Saving trained model to: /kaggle/working/poiss_cnn_n2n_nl15.pth\nModel saved successfully.\n","output_type":"stream"}],"execution_count":10},{"cell_type":"markdown","source":"## Section 3: Testing on a Single Image","metadata":{}},{"cell_type":"code","source":"# --- Parameters for Testing ---\nTEST_IMAGE_PATH = \"/kaggle/input/aiml-project/491.jpg\" # <-- CHANGE THIS TO YOUR TEST IMAGE PATH\nSAVED_MODEL_FILENAME = f\"{NOISE_TYPE}_cnn_n2n_nl{NOISE_LEVEL}.pth\" # Should match the saved model name from Section 2\nSAVED_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, SAVED_MODEL_FILENAME)\n\n# Set this based on your test image:\nTEST_IMAGE_IS_ALREADY_NOISY = False # If True, noise addition step below will be skipped\nTEST_NOISE_TYPE = NOISE_TYPE           # Noise type *if* adding noise to a clean test image\nTEST_NOISE_LEVEL = NOISE_LEVEL               # Noise level *if* adding noise to a clean test image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:28:36.943669Z","iopub.execute_input":"2025-04-13T05:28:36.943930Z","iopub.status.idle":"2025-04-13T05:28:36.956456Z","shell.execute_reply.started":"2025-04-13T05:28:36.943912Z","shell.execute_reply":"2025-04-13T05:28:36.955545Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# --- Load Saved Model ---\nprint(f\"Loading model from: {SAVED_MODEL_PATH}\")\n# Ensure the model architecture is defined (it should be if you run cells sequentially)\ntest_model = DenoisingCNN(n_chan=3).to(device)\ntry:\n    test_model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=device))\n    test_model.eval() # Set model to evaluation mode (important!)\n    print(\"Model loaded successfully.\")\nexcept FileNotFoundError:\n    print(f\"Error: Model file not found at {SAVED_MODEL_PATH}. Did Section 2 run correctly?\")\n    # Stop or handle error\n    raise FileNotFoundError(f\"Model file not found at {SAVED_MODEL_PATH}\")\nexcept Exception as e:\n     print(f\"Error loading model state_dict: {e}\")\n     raise e","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:28:36.957438Z","iopub.execute_input":"2025-04-13T05:28:36.957772Z","iopub.status.idle":"2025-04-13T05:28:36.984054Z","shell.execute_reply.started":"2025-04-13T05:28:36.957745Z","shell.execute_reply":"2025-04-13T05:28:36.983113Z"}},"outputs":[{"name":"stdout","text":"Loading model from: /kaggle/working/poiss_cnn_n2n_nl15.pth\nModel loaded successfully.\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/1946807162.py:6: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  test_model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=device))\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# --- Load and Prepare Test Image ---\nprint(f\"Loading test image: {TEST_IMAGE_PATH}\")\nif not os.path.exists(TEST_IMAGE_PATH):\n     print(f\"Error: Test image not found at {TEST_IMAGE_PATH}\")\n     raise FileNotFoundError(f\"Test image not found at {TEST_IMAGE_PATH}\")\n\n# Preprocess the image (loads as clean, potentially)\ninput_tensor = preprocess_image(TEST_IMAGE_PATH, IMG_SIZE)\n\nif input_tensor is None:\n    print(\"Failed to process test image.\")\n    raise ValueError(\"Test image processing failed.\")\n\ninput_tensor = input_tensor.to(device)\noriginal_for_display = input_tensor.cpu().clone() # Keep a copy for display before potential noising\n\n# --- Add Noise (if test image is clean) ---\nif not TEST_IMAGE_IS_ALREADY_NOISY:\n    print(f\"Adding {TEST_NOISE_TYPE} noise (level {TEST_NOISE_LEVEL}) to the test image.\")\n    noisy_input_tensor = add_noise(input_tensor, TEST_NOISE_TYPE, TEST_NOISE_LEVEL)\n    # IMPORTANT: Make sure the noise added here matches the type/level the model was trained on if you expect optimal results!\nelse:\n    print(\"Test image is assumed to be already noisy. Skipping noise addition.\")\n    noisy_input_tensor = input_tensor # Use the loaded tensor directly as the noisy input\n\n# Add batch dimension for the model\nnoisy_input_batch = noisy_input_tensor.unsqueeze(0)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:28:36.985012Z","iopub.execute_input":"2025-04-13T05:28:36.985617Z","iopub.status.idle":"2025-04-13T05:28:37.099364Z","shell.execute_reply.started":"2025-04-13T05:28:36.985568Z","shell.execute_reply":"2025-04-13T05:28:37.098672Z"}},"outputs":[{"name":"stdout","text":"Loading test image: /kaggle/input/aiml-project/491.jpg\nAdding poiss noise (level 15) to the test image.\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"# --- Perform Denoising ---\nprint(\"Denoising the image...\")\nwith torch.no_grad(): # Disable gradient calculation for inference\n    denoised_output_batch = test_model(noisy_input_batch)\n\n# Remove batch dimension and clamp output to valid range [0, 1]\ndenoised_output_tensor = torch.clamp(denoised_output_batch.squeeze(0), 0.0, 1.0)\nprint(\"Denoising complete.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# NOTEBOOK CELL 5 (Updated)\n\n# --- Calculate PSNR Values for Display ---\n# Ensure tensors used for comparison are on the correct device\n# 'original_for_display' was created as a CPU tensor, move it to the compute device\nclean_tensor_device = original_for_display.to(device)\n# 'noisy_input_tensor' and 'denoised_output_tensor' should already be on the compute device\n# from previous steps\n\n# Calculate PSNR for Noisy vs Clean\nmse_noisy = F.mse_loss(noisy_input_tensor, clean_tensor_device)\nif mse_noisy.item() > 1e-10: # Avoid log(0)\n    psnr_noisy = 10 * torch.log10(1.0 / mse_noisy).item()\nelse:\n    psnr_noisy = float('inf')\n\n# Calculate PSNR for Denoised vs Clean\nmse_denoised = F.mse_loss(denoised_output_tensor, clean_tensor_device)\nif mse_denoised.item() > 1e-10: # Avoid log(0)\n    psnr_denoised = 10 * torch.log10(1.0 / mse_denoised).item()\nelse:\n    psnr_denoised = float('inf')\n\nprint(f\"Displaying Images - PSNR (Noisy vs Clean): {psnr_noisy:.2f} dB\")\nprint(f\"Displaying Images - PSNR (Denoised vs Clean): {psnr_denoised:.2f} dB\")\n\n\n# --- Display Results ---\n\n# Move tensors to CPU and permute C,H,W to H,W,C for matplotlib\n# Use the already created CPU copy for the original image\noriginal_display_np = original_for_display.permute(1, 2, 0).numpy()\nnoisy_display_np = noisy_input_tensor.cpu().permute(1, 2, 0).numpy()\ndenoised_display_np = denoised_output_tensor.cpu().permute(1, 2, 0).numpy()\n\nfig, ax = plt.subplots(1, 3, figsize=(18, 7)) # Slightly increased height for titles\n\n# Display Original Image\nax[0].imshow(np.clip(original_display_np, 0, 1))\nax[0].set_title('Original Image')\nax[0].axis('off')\n\n# Display Noisy Image with PSNR\nax[1].imshow(np.clip(noisy_display_np, 0, 1))\n# Format title to include PSNR value\ntitle_noisy = f'Noisy Input Image\\nPSNR: {psnr_noisy:.2f} dB'\nax[1].set_title(title_noisy)\nax[1].axis('off')\n\n# Display Denoised Image with PSNR\nax[2].imshow(np.clip(denoised_display_np, 0, 1))\n# Format title to include PSNR value\ntitle_denoised = f'Denoised Output Image\\nPSNR: {psnr_denoised:.2f} dB'\nax[2].set_title(title_denoised)\nax[2].axis('off')\n\nplt.tight_layout(pad=1.5) # Add padding to prevent title overlap if needed\nplt.show()\n\n# Note: The optional PSNR calculation comparing to a separate ground truth\n# file is removed here as the primary request was to show PSNR vs the\n# clean input used in this cell's workflow.","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:28:56.070453Z","iopub.execute_input":"2025-04-13T05:28:56.070757Z"}},"outputs":[{"name":"stdout","text":"Displaying Images - PSNR (Noisy vs Clean): 9.33 dB\nDisplaying Images - PSNR (Denoised vs Clean): 10.46 dB\n","output_type":"stream"}],"execution_count":null},{"cell_type":"code","source":"# -*- coding: utf-8 -*-\nimport os\nimport time # Optional: for timing\nimport pandas as pd # For saving results to CSV\nimport numpy as np # For calculating average PSNR\n\nimport torch\nimport torch.nn as nn # Assuming DenoisingCNN uses this\nimport torch.nn.functional as F # For mse_loss\n\n# NOTE: Ensure necessary libraries like os, torch, pandas, numpy are imported.\n# Assume the following are defined earlier in your script:\n# - device = 'cuda' or 'cpu'\n# - DenoisingCNN class definition\n# - preprocess_image(image_path, img_size) function\n# - add_noise(image_tensor, noise_type, noise_level, device) function\n# - IMG_SIZE (e.g., 256)\n# - N_CHAN (e.g., 3)\n# - MODEL_SAVE_DIR (e.g., './models')\n# - NOISE_TYPE (e.g., 'gauss', matching the saved model)\n# - NOISE_LEVEL (e.g., 25, matching the saved model)\n\nN_CHAN = 3 # Number of channels in the images (RGB)\n\n# --- Configuration for Batch Testing ---\nTEST_IMG_DIR = '/kaggle/input/aiml-project/test_images_50' # Directory containing the 50 test images\nCSV_OUTPUT_FILENAME = f'{NOISE_TYPE}_cnn_n2n_nl{NOISE_LEVEL}.csv'\n\n# Noise parameters to *add* to the clean test images during this test run\n# It's crucial these match the conditions the loaded model expects for best results\nTEST_NOISE_TYPE = NOISE_TYPE      # Use the same noise type as the loaded model\nTEST_NOISE_LEVEL = NOISE_LEVEL    # Use the same noise level as the loaded model\n\n# --- Construct Model Path (using assumed variables) ---\nSAVED_MODEL_FILENAME = f\"{NOISE_TYPE}_cnn_n2n_nl{NOISE_LEVEL}.pth\"\nSAVED_MODEL_PATH = os.path.join(MODEL_SAVE_DIR, SAVED_MODEL_FILENAME)\n\n# --- Load Saved Model (Once) ---\nprint(f\"Loading model from: {SAVED_MODEL_PATH}\")\n# Instantiate the model architecture (ensure N_CHAN is correctly defined above)\ntest_model = DenoisingCNN(n_chan=N_CHAN).to(device)\ntry:\n    # Load the learned weights\n    test_model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=device))\n    test_model.eval() # Set model to evaluation mode (important!)\n    print(\"Model loaded successfully.\")\nexcept FileNotFoundError:\n    print(f\"Error: Model file not found at {SAVED_MODEL_PATH}. Ensure the path and filename are correct.\")\n    # Stop or handle error gracefully depending on your workflow\n    raise FileNotFoundError(f\"Model file not found at {SAVED_MODEL_PATH}\")\nexcept Exception as e:\n      print(f\"Error loading model state_dict: {e}\")\n      raise e # Re-raise the exception\n\n# --- Prepare for Batch Processing ---\nif not os.path.isdir(TEST_IMG_DIR):\n    print(f\"Error: Test image directory not found: {TEST_IMG_DIR}\")\n    raise FileNotFoundError(f\"Test image directory not found: {TEST_IMG_DIR}\")\n\n# Get list of image files (adjust extensions if needed)\nimage_extensions = ('.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff')\ntry:\n    all_files = os.listdir(TEST_IMG_DIR)\n    image_files = sorted([f for f in all_files if f.lower().endswith(image_extensions)])\nexcept Exception as e:\n    print(f\"Error listing files in {TEST_IMG_DIR}: {e}\")\n    raise\n\nif not image_files:\n    print(f\"No image files with extensions {image_extensions} found in {TEST_IMG_DIR}\")\n    exit() # Exit if no images to process\n\nprint(f\"Found {len(image_files)} images to process in {TEST_IMG_DIR}.\")\n\n# --- Initialize Results Storage ---\nresults_data = [] # List to hold {'filename': ..., 'psnr_db': ...} dictionaries\n\n# --- Process Each Image in the Directory ---\nprint(\"\\nStarting batch denoising and PSNR calculation...\")\nstart_time = time.time() # Optional: time the process\n\nfor filename in image_files:\n    current_image_path = os.path.join(TEST_IMG_DIR, filename)\n    # print(f\"Processing: {filename}...\") # Uncomment for more verbose output\n\n    try:\n        # --- Load and Prepare Test Image (as clean) ---\n        # Assumes preprocess_image loads, resizes to IMG_SIZE, returns [0,1] float CHW tensor\n        clean_input_tensor = preprocess_image(current_image_path, IMG_SIZE)\n        if clean_input_tensor is None:\n            print(f\"  Skipping {filename} due to preprocessing error.\")\n            results_data.append({'filename': filename, 'psnr_db': np.nan}) # Record error\n            continue # Skip to next image\n\n        clean_input_tensor = clean_input_tensor.to(device)\n\n        # --- Add Noise ---\n        # We assume TEST_IMAGE_IS_ALREADY_NOISY is False for all files in this batch test\n        noisy_input_tensor = add_noise(clean_input_tensor, TEST_NOISE_TYPE, TEST_NOISE_LEVEL)\n\n        # --- Prepare Batch for Model (B=1) ---\n        noisy_input_batch = noisy_input_tensor.unsqueeze(0)\n\n        # --- Perform Denoising ---\n        with torch.no_grad(): # Ensure no gradients are calculated\n            denoised_output_batch = test_model(noisy_input_batch)\n\n        # Remove batch dimension and clamp output to valid range [0, 1]\n        denoised_output_tensor = torch.clamp(denoised_output_batch.squeeze(0), 0.0, 1.0)\n\n        # --- Calculate PSNR (Denoised vs Original Clean) ---\n        # Ensure both tensors are on the same device (should be already)\n        mse_loss = F.mse_loss(denoised_output_tensor, clean_input_tensor)\n\n        # Calculate PSNR, handle potential zero MSE case\n        if mse_loss.item() > 1e-10: # Avoid division by zero or log(0)\n            # PSNR for images normalized to [0, 1] has max_val = 1.0\n            psnr = 10 * torch.log10(1.0 / mse_loss)\n            current_psnr = psnr.item()\n        else:\n            current_psnr = float('inf') # Assign infinity for perfect reconstruction\n\n        # print(f\"  PSNR: {current_psnr:.2f} dB\") # Uncomment for verbose output\n\n        # --- Store Result ---\n        results_data.append({'filename': filename, 'psnr_db': current_psnr})\n\n    except Exception as e:\n        print(f\"  Error processing {filename}: {e}\")\n        # Store NaN or another indicator for the error case\n        results_data.append({'filename': filename, 'psnr_db': np.nan})\n        # import traceback # Optional: for debugging\n        # traceback.print_exc() # Optional: prints traceback\n\n# --- End of Loop ---\nend_time = time.time() # Optional\nprint(f\"\\nBatch processing completed in {end_time - start_time:.2f} seconds.\")\n\n# --- Calculate Average PSNR and Save Results ---\nif results_data:\n    # Convert results list to a pandas DataFrame\n    results_df = pd.DataFrame(results_data)\n\n    # Calculate average PSNR\n    # Exclude NaN (errors) and Inf (perfect reconstruction) values for a meaningful average\n    valid_psnr_values = results_df['psnr_db'].replace([np.inf, -np.inf], np.nan).dropna()\n\n    if not valid_psnr_values.empty:\n        average_psnr = valid_psnr_values.mean()\n        print(f\"\\nAverage PSNR over {len(valid_psnr_values)} successfully processed images: {average_psnr:.2f} dB\")\n    else:\n        average_psnr = np.nan # Indicate no valid results if all failed or were Inf\n        print(\"\\nCould not calculate average PSNR (no valid results).\")\n\n    # Save the full results DataFrame to CSV\n    try:\n        results_df.to_csv(CSV_OUTPUT_FILENAME, index=False)\n        print(f\"Results saved to {CSV_OUTPUT_FILENAME}\")\n    except Exception as e:\n        print(f\"Error saving results to CSV ({CSV_OUTPUT_FILENAME}): {e}\")\nelse:\n    print(\"No images were processed.\")\n\nprint(\"\\nScript finished.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-13T05:28:38.108175Z","iopub.execute_input":"2025-04-13T05:28:38.108552Z","iopub.status.idle":"2025-04-13T05:28:39.088315Z","shell.execute_reply.started":"2025-04-13T05:28:38.108519Z","shell.execute_reply":"2025-04-13T05:28:39.087376Z"}},"outputs":[{"name":"stdout","text":"Loading model from: /kaggle/working/poiss_cnn_n2n_nl15.pth\nModel loaded successfully.\nFound 50 images to process in /kaggle/input/aiml-project/test_images_50.\n\nStarting batch denoising and PSNR calculation...\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_31/163305715.py:44: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n  test_model.load_state_dict(torch.load(SAVED_MODEL_PATH, map_location=device))\n","output_type":"stream"},{"name":"stdout","text":"\nBatch processing completed in 0.61 seconds.\n\nAverage PSNR over 50 successfully processed images: 24.66 dB\nResults saved to poiss_cnn_n2n_nl15.csv\n\nScript finished.\n","output_type":"stream"}],"execution_count":16}]}